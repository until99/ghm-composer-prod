#!/usr/bin/env python3
"""
Script de valida√ß√£o para pipelines de dados.

Valida:
- Configura√ß√µes de datasets
- Depend√™ncias entre pipelines
- Arquivos SQL existem
- Sintaxe das DAGs
"""

import sys
from pathlib import Path
from typing import List, Tuple

# Adiciona o diret√≥rio raiz ao path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils import PIPELINES, get_pipeline_config


class ValidationError:
    def __init__(self, dag_id: str, error_type: str, message: str):
        self.dag_id = dag_id
        self.error_type = error_type
        self.message = message

    def __str__(self):
        return f"‚ùå [{self.dag_id}] {self.error_type}: {self.message}"


class ValidationWarning:
    def __init__(self, dag_id: str, warning_type: str, message: str):
        self.dag_id = dag_id
        self.warning_type = warning_type
        self.message = message

    def __str__(self):
        return f"‚ö†Ô∏è  [{self.dag_id}] {self.warning_type}: {self.message}"


def validate_dataset_config(dag_id: str, config) -> List[ValidationError]:
    """Valida configura√ß√£o do dataset."""
    errors = []

    # Valida dataset principal
    if not config.dataset:
        errors.append(ValidationError(dag_id, "CONFIG", "Dataset n√£o configurado"))
        return errors

    # Valida campos obrigat√≥rios
    if not config.dataset.project_id:
        errors.append(ValidationError(dag_id, "DATASET", "project_id n√£o definido"))

    if not config.dataset.dataset:
        errors.append(ValidationError(dag_id, "DATASET", "dataset n√£o definido"))

    if not config.dataset.table:
        errors.append(ValidationError(dag_id, "DATASET", "table n√£o definida"))

    if not config.dataset.layer or config.dataset.layer not in [
        "bronze",
        "silver",
        "gold",
    ]:
        errors.append(
            ValidationError(
                dag_id, "DATASET", f"layer inv√°lida: {config.dataset.layer}"
            )
        )

    return errors


def validate_dependencies(
    dag_id: str, config
) -> Tuple[List[ValidationError], List[ValidationWarning]]:
    """Valida depend√™ncias entre pipelines."""
    errors = []
    warnings = []

    # Bronze n√£o deve ter depend√™ncias
    if config.dataset.layer == "bronze" and config.dependencies:
        warnings.append(
            ValidationWarning(
                dag_id,
                "DEPENDENCY",
                "Bronze layer com depend√™ncias (esperado: sem depend√™ncias)",
            )
        )

    # Silver/Gold devem ter depend√™ncias
    if config.dataset.layer in ["silver", "gold"] and not config.dependencies:
        warnings.append(
            ValidationWarning(
                dag_id,
                "DEPENDENCY",
                f"{config.dataset.layer.capitalize()} layer sem depend√™ncias (esperado: com depend√™ncias)",
            )
        )

    # Valida cada depend√™ncia
    if config.dependencies:
        for dep in config.dependencies:
            if not dep.full_table_id:
                errors.append(
                    ValidationError(
                        dag_id, "DEPENDENCY", f"Depend√™ncia inv√°lida: {dep}"
                    )
                )

    return errors, warnings


def validate_sql_file(dag_id: str, layer: str) -> List[ValidationError]:
    """Valida se arquivo SQL existe."""
    errors = []

    sql_path = project_root / "sql" / f"{dag_id}.sql"

    if not sql_path.exists():
        errors.append(
            ValidationError(dag_id, "SQL", f"Arquivo n√£o encontrado: {sql_path}")
        )

    return errors


def validate_dag_file(dag_id: str) -> List[ValidationError]:
    """Valida se arquivo de DAG existe e tem sintaxe v√°lida."""
    errors = []

    dag_path = project_root / "dags" / f"{dag_id}.py"

    if not dag_path.exists():
        errors.append(
            ValidationError(dag_id, "DAG", f"Arquivo n√£o encontrado: {dag_path}")
        )
        return errors

    # Tenta compilar o arquivo Python
    try:
        with open(dag_path, "r", encoding="utf-8") as f:
            compile(f.read(), str(dag_path), "exec")
    except SyntaxError as e:
        errors.append(ValidationError(dag_id, "DAG", f"Erro de sintaxe: {e}"))

    return errors


def validate_schedule(dag_id: str, config) -> List[ValidationWarning]:
    """Valida configura√ß√£o de schedule."""
    warnings = []

    # Bronze deve ter schedule
    if config.dataset.layer == "bronze" and not config.schedule_interval:
        warnings.append(
            ValidationWarning(
                dag_id,
                "SCHEDULE",
                "Bronze sem schedule_interval (deveria ter @daily, @hourly, etc)",
            )
        )

    # Silver/Gold n√£o devem ter schedule (usam dataset triggers)
    if config.dataset.layer in ["silver", "gold"] and config.schedule_interval:
        warnings.append(
            ValidationWarning(
                dag_id,
                "SCHEDULE",
                f"{config.dataset.layer.capitalize()} com schedule_interval (deveria ser None para dataset trigger)",
            )
        )

    return warnings


def run_validation():
    """Executa todas as valida√ß√µes."""
    print("\n" + "=" * 60)
    print("üîç VALIDANDO CONFIGURA√á√ÉO DE PIPELINES")
    print("=" * 60 + "\n")

    all_errors = []
    all_warnings = []

    for dag_id, config in PIPELINES.items():
        print(f"Validando: {dag_id} ({config.dataset.layer})...")

        # Valida√ß√µes
        all_errors.extend(validate_dataset_config(dag_id, config))

        dep_errors, dep_warnings = validate_dependencies(dag_id, config)
        all_errors.extend(dep_errors)
        all_warnings.extend(dep_warnings)

        all_errors.extend(validate_sql_file(dag_id, config.dataset.layer))
        all_errors.extend(validate_dag_file(dag_id))
        all_warnings.extend(validate_schedule(dag_id, config))

    # Resultados
    print("\n" + "=" * 60)
    print("üìä RESULTADOS DA VALIDA√á√ÉO")
    print("=" * 60 + "\n")

    if all_errors:
        print(f"‚ùå {len(all_errors)} ERRO(S) ENCONTRADO(S):\n")
        for error in all_errors:
            print(f"  {error}")
        print()

    if all_warnings:
        print(f"‚ö†Ô∏è  {len(all_warnings)} AVISO(S):\n")
        for warning in all_warnings:
            print(f"  {warning}")
        print()

    if not all_errors and not all_warnings:
        print("‚úÖ TODAS AS VALIDA√á√ïES PASSARAM!")
        print("üöÄ Pipeline est√° pronto para deploy!")
    elif not all_errors:
        print("‚úÖ Sem erros cr√≠ticos")
        print("‚ö†Ô∏è  Revise os avisos antes do deploy")
    else:
        print("‚ùå CORRIJA OS ERROS ANTES DO DEPLOY")
        return 1

    print()
    return 0


def print_pipeline_summary():
    """Imprime resumo das pipelines configuradas."""
    print("\n" + "=" * 60)
    print("üìã RESUMO DAS PIPELINES")
    print("=" * 60 + "\n")

    layers = {"bronze": [], "silver": [], "gold": []}

    for dag_id, config in PIPELINES.items():
        layers[config.dataset.layer].append(dag_id)

    for layer, dags in layers.items():
        print(f"{layer.upper()} Layer ({len(dags)} DAGs):")
        for dag in sorted(dags):
            config = PIPELINES[dag]
            schedule = config.schedule_interval or "dataset-triggered"
            deps = len(config.dependencies) if config.dependencies else 0
            print(f"  ‚îú‚îÄ {dag}")
            print(f"  ‚îÇ  ‚îú‚îÄ Schedule: {schedule}")
            print(f"  ‚îÇ  ‚îî‚îÄ Dependencies: {deps}")
        print()


if __name__ == "__main__":
    print_pipeline_summary()
    exit_code = run_validation()
    sys.exit(exit_code)
